# End-to-End Development Test Runs

This document describes the complete workflow for running repeatable end-to-end tests of the SmartEM system using pre-recorded microscope sessions. It serves as both a runbook for executing tests and a reference for understanding test requirements.

## Agent Versions

SmartEM Decisions includes two agent implementations:

- **Agent V1** (`smartem_agent`): Original implementation
- **Agent V2** (`smartem_agent2`): Rewritten architecture with improved state management and processing pipeline

Both agents provide the same core functionality but differ in internal architecture. Agent V2 represents the current development
direction with enhanced modularity and maintainability.

## Quick Start (Automated Test Runner)

For the simplest test execution, use the automated test runner scripts:

### Agent V1 (Original)

```bash
# Check k3s status first (DO NOT restart if already running)
./tools/dev-k8s.sh status

# Run automated test (Test Type 2: Pre-Acquisition Agent Setup)
./tools/run-e2e-test.sh

# With custom parameters
./tools/run-e2e-test.sh \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  /home/username/dev/DLS/epu-test-dir \
  0.1
```

### Agent V2 (Rewrite)

```bash
# Check k3s status first (DO NOT restart if already running)
./tools/dev-k8s.sh status

# Run automated test with agent v2
./tools/run-e2e-test-v2.sh

# With custom parameters
./tools/run-e2e-test-v2.sh \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  /home/username/dev/DLS/epu-test-dir \
  0.1
```

**Script parameters** (all optional, both scripts):
1. Recording file path (default: `~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz`)
2. EPU output directory (default: `~/dev/DLS/epu-test-dir`)
3. Max delay in seconds (default: `0.1`)

**What the scripts do**:
- Creates timestamped test results directory in `logs/e2e-tests/`
- Activates venv and loads environment variables from `.env` (v1) or `.env.local-test-run` (v2)
- Resets database to clean state
- Starts API server and consumer
- Starts agent watching empty directory (with `--use-v2` flag for v2 script)
- Runs playback with specified timing
- Collects filesystem and database statistics
- Saves all logs to test results directory
- Cleans up background processes on exit

**When to use**:
- Quick smoke tests during development
- Consistent repeatable test runs
- Pre-Acquisition scenario testing (Test Type 2)
- Comparing behaviour between agent versions

**When to use manual execution instead**:
- Testing Post-Acquisition scenario (Test Type 1)
- Testing Mid-Acquisition scenario (Test Type 3)
- Debugging specific service interactions
- Custom timing or service startup order

## Quick Start (Manual Execution)

If you've already set up once and just need to run another test:

```bash
# 1. Check k3s status (DO NOT restart if already running)
./tools/dev-k8s.sh status

# 2. Prepare clean environment
unset POSTGRES_HOST POSTGRES_PORT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD
unset RABBITMQ_HOST RABBITMQ_PORT RABBITMQ_USER RABBITMQ_PASSWORD
unset HTTP_API_HOST HTTP_API_PORT
set -a && source .env && set +a

# 3. Create timestamped test directory
TEST_DIR="logs/e2e-tests/$(date +%Y-%m-%d_%H%M%S)_test-type-name"
mkdir -p "$TEST_DIR/logs"

# 4. Kill any running processes
pkill -f "smartem_backend\|smartem_agent\|fsrecorder\|uvicorn\|fastapi" || true

# 5. Restart RabbitMQ (ensures empty queue)
kubectl rollout restart deployment/rabbitmq -n smartem-decisions
kubectl rollout status deployment/rabbitmq -n smartem-decisions

# 6. Reset database
source .venv/bin/activate
python -m smartem_backend.model.database

# 7. Clean playback directory
rm -rf ../epu-test-dir && mkdir -p ../epu-test-dir

# 8. Start services (choose a test type below, or see Test Execution Scenarios section)
# Example: Test Type 1 - Post-acquisition
python -m uvicorn smartem_backend.api_server:app --host 127.0.0.1 --port 8000 --log-level debug 2>&1 | tee "$TEST_DIR/logs/api.log" &
python -m smartem_backend.consumer -vv 2>&1 | tee "$TEST_DIR/logs/consumer.log" &
sleep 3

# Run playback (default is --fast mode, no flag needed)
python ./tools/fsrecorder/fsrecorder.py replay \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/playback.log"

# Start agent after playback completes
python -m smartem_agent watch --api-url http://localhost:8000 -vv ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/agent.log"
```

**For detailed explanations, first-time setup, or different test scenarios, see sections below.**

---

## Overview

The test setup simulates a complete SmartEM workflow:
- **Local k3s cluster**: Runs PostgreSQL and RabbitMQ services
- **Host OS services**: Runs SmartEM backend API and worker for easier debugging
- **Microscope simulation**: Plays back pre-recorded microscopy sessions
- **Agent monitoring**: SmartEM agent watches for file changes and processes data

## Prerequisites

### Environment Setup
- Python 3.12+ with venv activated: `source .venv/bin/activate`
- Full development install: `pip install -e .[all]`
- Local k3s cluster running: `./tools/dev-k8s.sh up`
- Environment file: `.env` (created from `.env.example` - see Environment File Setup below)

### Test Data
Pre-recorded microscope sessions are stored in `~/dev/DLS/smartem-decisions-test-recordings/`:
- `bi37600-29_fsrecord.tar.gz`
- `bi37708-42_fsrecord.tar.gz` (recommended for testing, 8389 events)

**Note**: Path should be absolute or relative to working directory (`smartem-decisions/`)

### Directory Structure
```
logs/e2e-tests/                  # Test results root (gitignored, at repo root)
  YYYY-MM-DD_HHmmss_test-type/   # Individual test run
    logs/                        # All log outputs (4 services)
      playback.log               # Microscope recording playback
      agent.log                  # SmartEM agent
      api.log                    # Backend HTTP API
      consumer.log               # Backend worker/consumer
    db-dump.sql                  # Database state after test
    test-params.json             # Test configuration used
```

### Environment File Setup

The project uses different environment files depending on the agent version being tested:

#### Agent V1 Environment Configuration

The `.env` file configures services to run on host OS whilst connecting to k3s infrastructure.

**Source for initial values**: Copy from `.env.example` and configure credentials

**Required configuration**:
- PostgreSQL: Point to k3s NodePort `localhost:30432`
- RabbitMQ: Point to k3s NodePort `localhost:30672`
- Backend API: Run on host OS (typically `127.0.0.1:8000`)
- Agent: Run on host OS

**Example `.env`**:
```bash
# Database connection (k3s NodePort)
POSTGRES_HOST=localhost
POSTGRES_PORT=30432
POSTGRES_DB=smartem_db
POSTGRES_USER=username
POSTGRES_PASSWORD=password

# RabbitMQ connection (k3s NodePort)
RABBITMQ_HOST=localhost
RABBITMQ_PORT=30672
RABBITMQ_USER=guest
RABBITMQ_PASSWORD=guest

# Backend API (host OS)
HTTP_API_HOST=127.0.0.1
HTTP_API_PORT=8000
```

**Usage**: `source .env` before starting services

#### Agent V2 Environment Configuration

The `.env.local-test-run` file is used specifically for agent v2 testing.

**Source for initial values**: Copy from `.env.example.local-test-run` and configure credentials

**Configuration matches agent v1** with the same NodePort mappings and service endpoints. The separate file allows for
isolated configuration when testing the new agent implementation.

**Example `.env.local-test-run`**:
```bash
# Database configuration (connecting to K8s NodePort)
POSTGRES_HOST=localhost
POSTGRES_PORT=30432
POSTGRES_DB=smartem_db
POSTGRES_USER=username
POSTGRES_PASSWORD=password

# RabbitMQ configuration (connecting to K8s NodePort)
RABBITMQ_HOST=localhost
RABBITMQ_PORT=30672
RABBITMQ_UI_PORT=15672
RABBITMQ_USER=username
RABBITMQ_PASSWORD=password

# HTTP API configuration (running on host OS)
HTTP_API_PORT=8000
ADMINER_PORT=8080

# CORS Configuration
CORS_ALLOWED_ORIGINS=*
```

**Usage**: Automatically loaded by `./tools/run-e2e-test-v2.sh` script

## Database Operations

### Creating Fresh Database
Run `database.py` to drop all tables and recreate schema with indexes:
```bash
source .venv/bin/activate
python -m smartem_backend.model.database
```

**Note**: This script uses `_create_db_and_tables()` which:
1. Drops all existing tables and custom enum types
2. Creates all tables from SQLModel definitions
3. Creates all indexes for optimal query performance
4. Does NOT require Alembic migrations

### Connecting to Database
From host OS (using k3s NodePort mappings):
```bash
# Connection details (from .env)
POSTGRES_HOST=localhost
POSTGRES_PORT=30432
POSTGRES_DB=smartem_db
POSTGRES_USER=username
POSTGRES_PASSWORD=password

# Example: Dump database for test results
pg_dump -h localhost -p 30432 -U username -d smartem_db > logs/e2e-tests/TIMESTAMP/db-dump.sql
```

## RabbitMQ Queue Management

### Queue Behaviour
- Queue name: `smartem_backend` (configured in `src/smartem_backend/appconfig.yml`)
- Queue is declared with `durable=True` by both publisher and consumer
- Queue persists across reconnections but NOT across RabbitMQ service restarts
- **Important**: Queue is NOT recreated empty on each connection - existing messages persist

### Verifying Queue State
Access RabbitMQ management UI:
```bash
# RabbitMQ management interface (if enabled in k3s)
# Check k8s service for management UI port mapping
# Default credentials: guest/guest  # pragma: allowlist secret
```

### Cleaning Queue Between Tests
**Option 1**: Restart RabbitMQ service (recommended for clean state):
```bash
kubectl rollout restart deployment/rabbitmq -n smartem-decisions
kubectl rollout status deployment/rabbitmq -n smartem-decisions
```

**Option 2**: Purge queue programmatically (if needed):
```python
# Add to teardown script if queue purging is required
# This would need to be implemented in utils.py
```

## Service Commands

**IMPORTANT - Environment Variables**:
All services use `load_dotenv(override=False)` which loads `.env` by default but doesn't override already-set variables.
For testing, you MUST export variables from `.env` before starting services:

```bash
# Unset any previously loaded env vars to avoid conflicts
unset POSTGRES_HOST POSTGRES_PORT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD
unset RABBITMQ_HOST RABBITMQ_PORT RABBITMQ_USER RABBITMQ_PASSWORD
unset HTTP_API_HOST HTTP_API_PORT

# Load test environment (exports all variables)
set -a
source .env
set +a
```

### Backend HTTP API

**RECOMMENDED: Uvicorn without auto-reload (for e2e tests)**
```bash
source .venv/bin/activate
python -m uvicorn smartem_backend.api_server:app \
  --host 127.0.0.1 \
  --port 8000 \
  --log-level debug 2>&1 | tee logs/e2e-tests/TIMESTAMP/logs/api.log
```
- **Log level options**: `critical`, `error`, `warning`, `info`, `debug`, `trace`
- No auto-reload to avoid file watch limits (important for repos with .tox directories)
- More verbose logging control
- Stable for long-running tests

**Alternative: FastAPI CLI (NOT recommended for e2e tests)**
```bash
source .venv/bin/activate
python -m fastapi dev src/smartem_backend/api_server.py \
  --host 127.0.0.1 \
  --port 8000 2>&1 | tee /tmp/e2e-test-results/TIMESTAMP/logs/api.log
```
- Auto-reload on code changes (watches entire project directory)
- **WARNING**: Will fail with "OS file watch limit reached" error if .tox or other large directories exist
- Only use for quick manual development, not for e2e test runs

**Note**: Both read database and RabbitMQ connection details from environment variables

### Backend Worker/Consumer
```bash
source .venv/bin/activate
export RABBITMQ_URL=amqp://guest:guest@localhost:30672/  # pragma: allowlist secret
python -m smartem_backend.consumer -vv 2>&1 | tee logs/e2e-tests/TIMESTAMP/logs/consumer.log
```
**Verbosity flags**: `-v` for INFO, `-vv` for DEBUG

### SmartEM Agent

#### Agent V1 (Original)
```bash
source .venv/bin/activate
python -m smartem_agent watch \
  --api-url http://localhost:30080 \
  --verbose -v \
  ../epu-test-dir 2>&1 | tee logs/e2e-tests/TIMESTAMP/logs/agent.log
```

#### Agent V2 (Rewrite)
```bash
source .venv/bin/activate
python -m smartem_agent watch \
  --api-url http://localhost:30080 \
  --use-v2 \
  --verbose -v \
  ../epu-test-dir 2>&1 | tee logs/e2e-tests/TIMESTAMP/logs/agent.log
```

**Verbosity flags** (both versions): `-v` for INFO, `-vv` for DEBUG

**Agent options** (both versions):
- `--agent-id TEXT`: Agent identifier for SSE connection
- `--session-id TEXT`: Session identifier for SSE connection
- `--sse-timeout INTEGER`: SSE connection timeout in seconds (default: 30)
- `--heartbeat-interval INTEGER`: Agent heartbeat interval in seconds (default: 60)
- `--use-v2`: Use agent v2 implementation (smartem_agent2 package)

### Microscope Playback (fsrecorder)

**Tool location**: `./tools/fsrecorder/fsrecorder.py` (relative to repo root)

**Command syntax**:
```bash
source .venv/bin/activate
python ./tools/fsrecorder/fsrecorder.py replay \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  ../epu-test-dir 2>&1 | tee logs/e2e-tests/TIMESTAMP/logs/playback.log
```

**Important**:
- Use `replay` subcommand (NOT `--mode replay` or `--mode player`)
- First positional argument is the recording file (`.tar.gz`, not `.events`)
- Second positional argument is the target directory (no trailing slash needed)
- Recording file path should be absolute (`~/dev/DLS/...`) or relative to current directory

**Speed control options** (from `--help`):
- Default behavior: `--fast` mode (100x speed, 1s max delays) - **enabled by default**
- `--dev-mode`: Maximum speed for rapid iteration (1000x + burst) - quick smoke tests
- `--fast`: Balanced acceleration (100x + 1s delays) - same as default
- `--exact`: Preserve original timing (1x speed) - debug timing issues
- `-s SPEED, --speed SPEED`: Custom speed multiplier
- `--max-delay MAX_DELAY`: Maximum delay between events in seconds
- `--burst`: Process events as fast as possible

**Additional options**:
- `--no-verify`: Skip integrity verification
- `--skip-unreadable`: Skip files that were unreadable during recording

**Example usage**:
```bash
# Default fast mode (no flag needed)
python ./tools/fsrecorder/fsrecorder.py replay recording.tar.gz ../output-dir

# Dev mode for rapid testing
python ./tools/fsrecorder/fsrecorder.py replay --dev-mode recording.tar.gz ../output-dir

# Exact timing for debugging
python ./tools/fsrecorder/fsrecorder.py replay --exact recording.tar.gz ../output-dir
```

## Test Preparation Workflow

### 1. Backup Previous Test Logs
```bash
# If previous test exists, logs are already in logs/e2e-tests/PREVIOUS_TIMESTAMP/
# No action needed - each test creates its own timestamped directory
```

### 2. Prepare Test Environment
```bash
# 1. Unset any previously loaded environment variables
unset POSTGRES_HOST POSTGRES_PORT POSTGRES_DB POSTGRES_USER POSTGRES_PASSWORD
unset RABBITMQ_HOST RABBITMQ_PORT RABBITMQ_USER RABBITMQ_PASSWORD
unset HTTP_API_HOST HTTP_API_PORT

# 2. Load test environment variables (export all)
set -a
source .env
set +a

# 3. Create test results directory with timestamp
TEST_DIR="logs/e2e-tests/$(date +%Y-%m-%d_%H%M%S)_test-type-name"
mkdir -p "$TEST_DIR/logs"

# 4. Stop any running services from previous test
pkill -f smartem_backend.api
pkill -f smartem_backend.consumer
pkill -f smartem_agent
pkill -f fsrecorder
pkill -f uvicorn
pkill -f fastapi

# 5. Verify k3s services are running (DO NOT restart if already running)
./tools/dev-k8s.sh status

# 6. Clean RabbitMQ (restart to ensure empty queue)
kubectl rollout restart deployment/rabbitmq -n smartem-decisions
kubectl rollout status deployment/rabbitmq -n smartem-decisions

# 7. Drop and recreate database
source .venv/bin/activate
python -m smartem_backend.model.database

# 8. Clean playback output directory
rm -rf ../epu-test-dir
mkdir -p ../epu-test-dir
```

### 3. Verify Prerequisites
```bash
# Check test recordings exist
ls -lh ~/dev/DLS/smartem-decisions-test-recordings/

# Verify k3s services accessible
curl -s http://localhost:30080/status || echo "API not ready yet"

# Check database connectivity
psql -h localhost -p 30432 -U username -d smartem_db -c "SELECT version();"
```

## Test Execution Scenarios

**CRITICAL**: Agent command is `python -m smartem_agent watch` (NOT `python -m smartem_agent.main`!)

**Note**: All test scenarios below can use either agent v1 (default) or agent v2 (add `--use-v2` flag). Examples show v1
syntax; add `--use-v2` after the `watch` subcommand to test agent v2.

### Test Type 1: Post-Acquisition Data Intake
**Scenario**: Agent starts watching after playback completes (easiest, should work)

```bash
# 1. Start backend services (ensure env vars are already exported from prep step)
python -m uvicorn smartem_backend.api_server:app --host 127.0.0.1 --port 8000 --log-level debug 2>&1 | tee "$TEST_DIR/logs/api.log" &
python -m smartem_backend.consumer -vv 2>&1 | tee "$TEST_DIR/logs/consumer.log" &

# Wait a moment for services to start
sleep 3

# 2. Run playback to completion (no agent watching yet)
# NOTE: Recording must be a .tar.gz file (e.g., bi37708-42_fsrecord.tar.gz)
python ./tools/fsrecorder/fsrecorder.py replay \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/playback.log"

# 3. Wait for playback to finish, then start agent
# IMPORTANT: Use "python -m smartem_agent watch" NOT "python -m smartem_agent.main"
python -m smartem_agent watch \
  --api-url http://localhost:8000 \
  -vv \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/agent.log"
```

### Test Type 2: Pre-Acquisition Agent Setup
**Scenario**: Agent starts watching before any data exists

```bash
# 1. Start backend services
python -m uvicorn smartem_backend.api_server:app --host 127.0.0.1 --port 8000 --log-level debug 2>&1 | tee "$TEST_DIR/logs/api.log" &
python -m smartem_backend.consumer -vv 2>&1 | tee "$TEST_DIR/logs/consumer.log" &

# Wait a moment for services to start
sleep 3

# 2. Start agent watching empty directory
# IMPORTANT: Use "python -m smartem_agent watch" NOT "python -m smartem_agent.main"
python -m smartem_agent watch \
  --api-url http://localhost:8000 \
  -vv \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/agent.log" &

# 3. Begin playback after agent is watching
# NOTE: Recording must be a .tar.gz file (e.g., bi37708-42_fsrecord.tar.gz)
python ./tools/fsrecorder/fsrecorder.py replay \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/playback.log"
```

### Test Type 3: Mid-Acquisition Agent Start
**Scenario**: Agent starts after playback begins but before completion (tests handling existing + new data)

**Timing considerations**:
- Playback script initially extracts data from recording (no files written yet)
- After extraction, playback begins writing files to output directory
- Agent should start after some files exist but before playback completes
- Use different delays to test at various stages of data availability

```bash
# 1. Start backend services
python -m uvicorn smartem_backend.api_server:app --host 127.0.0.1 --port 8000 --log-level debug 2>&1 | tee "$TEST_DIR/logs/api.log" &
python -m smartem_backend.consumer -vv 2>&1 | tee "$TEST_DIR/logs/consumer.log" &

# Wait a moment for services to start
sleep 3

# 2. Start playback first
# NOTE: Recording must be a .tar.gz file (e.g., bi37708-42_fsrecord.tar.gz)
python ./tools/fsrecorder/fsrecorder.py replay \
  ~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/playback.log" &

# 3. Monitor file creation to choose optimal start time
watch -n 1 "find ../epu-test-dir -type f | wc -l"

# 4. When desired number of files exist, start agent
# Experiment with different timing:
#   - sleep 5   # Very early (minimal files)
#   - sleep 15  # Early-mid stage
#   - sleep 30  # Mid-stage
#   - sleep 60  # Late stage
sleep 15  # Adjust based on monitoring above

# IMPORTANT: Use "python -m smartem_agent watch" NOT "python -m smartem_agent.main"
python -m smartem_agent watch \
  --api-url http://localhost:8000 \
  -vv \
  ../epu-test-dir 2>&1 | tee "$TEST_DIR/logs/agent.log"
```

## Common Mistakes to Avoid

**ALWAYS check the agent log file first if things aren't working!**

1. **Wrong agent command**: Use `python -m smartem_agent watch`, NOT:
   - ❌ `python -m smartem_agent.main`
   - ❌ `python -m smartem_agent`
   - ✅ `python -m smartem_agent watch` (agent v1)
   - ✅ `python -m smartem_agent watch --use-v2` (agent v2)

2. **Wrong recording path**: Must use .tar.gz file, NOT directory:
   - ❌ `~/dev/DLS/smartem-decisions-test-recordings/pre-acquisition`
   - ✅ `~/dev/DLS/smartem-decisions-test-recordings/bi37708-42_fsrecord.tar.gz`

3. **FastAPI dev server**: Use uvicorn for e2e tests, NOT `python -m fastapi dev`:
   - ❌ `python -m fastapi dev src/smartem_backend/api_server.py`
   - ✅ `python -m uvicorn smartem_backend.api_server:app`

4. **Forgot to activate venv**: All commands must run with venv activated:
   - ✅ `source .venv/bin/activate` at the start

5. **Agent log is tiny (< 1KB)**: Agent failed to start - check the error message in agent.log

## Post-Test Data Collection

After test completion, capture all relevant data for analysis:

```bash
# 1. Verify filesystem vs database counts
echo "=== Filesystem Counts ==="
echo "EPU Sessions (Supervisor_* dirs): $(ls -1d ../epu-test-dir/Supervisor_* 2>/dev/null | wc -l)"
echo "GridSquare directories: $(find ../epu-test-dir -type d -name "GridSquare_*" | wc -l)"
echo "FoilHole directories: $(find ../epu-test-dir -type d -name "FoilHole_*" | wc -l)"
echo "MRC files (micrographs): $(find ../epu-test-dir -type f -name "*.mrc" | wc -l)"
echo "XML metadata files: $(find ../epu-test-dir -type f -name "*.xml" | wc -l)"
echo ""
echo "=== Database Counts ==="
kubectl exec -n smartem-decisions deployment/postgres -- psql -U username -d smartem_db -c \
  "SELECT 'acquisition' as table_name, COUNT(*) as count FROM acquisition
   UNION ALL SELECT 'grid', COUNT(*) FROM grid
   UNION ALL SELECT 'gridsquare', COUNT(*) FROM gridsquare
   UNION ALL SELECT 'foilhole', COUNT(*) FROM foilhole
   UNION ALL SELECT 'micrograph', COUNT(*) FROM micrograph
   ORDER BY table_name;"
echo ""
echo "=== Agent Processing Issues ==="
echo "No grid found messages: $(grep -c 'No grid found' "$TEST_DIR/logs/agent.log" 2>/dev/null || echo 0)"

# 2. Database dump (final state after all processing)
kubectl exec -n smartem-decisions deployment/postgres -- pg_dump -U username -d smartem_db > "$TEST_DIR/db-dump.sql"

# 3. Create test parameters record
cat > "$TEST_DIR/test-params.json" << EOF
{
  "test_type": "post-acquisition",
  "timestamp": "$(date -Iseconds)",
  "recording": "bi37708-42_fsrecord.tar.gz",
  "playback_mode": "fast",
  "agent_verbosity": "debug",
  "consumer_verbosity": "debug",
  "api_log_level": "debug",
  "notes": "Testing data intake after microscope session completes"
}
EOF

# 4. Verify all 4 service logs were captured
ls -lh "$TEST_DIR/logs/"
# Expected files:
#   - playback.log  (microscope recording playback)
#   - agent.log     (SmartEM agent)
#   - api.log       (backend HTTP API)
#   - consumer.log  (backend worker/consumer)

# 5. Optional: Capture k8s infrastructure logs if debugging issues
kubectl logs deployment/postgres -n smartem-decisions > "$TEST_DIR/logs/postgres.log"
kubectl logs deployment/rabbitmq -n smartem-decisions > "$TEST_DIR/logs/rabbitmq.log"
```

**Critical data for analysis**:
1. **Filesystem vs Database counts**: Verify data ingestion completeness
2. **playback.log**: File creation timeline, extraction timing
3. **agent.log**: File detection, API communication, SSE events, "No grid found" errors
4. **api.log**: HTTP requests, database writes, SSE connections
5. **consumer.log**: RabbitMQ message processing, prediction updates
6. **db-dump.sql**: Final database state (acquisitions, grids, predictions)

## Monitoring and Verification

### During Test Execution

**Check file creation**:
```bash
watch -n 2 "find ../epu-test-dir -type f | wc -l"
```

**Monitor API status**:
```bash
# Check acquisitions being created
curl -s http://localhost:8000/acquisitions | jq

# Check overall status
curl -s http://localhost:8000/status | jq
```

**Watch logs in real-time**:
```bash
tail -f "$TEST_DIR/logs/agent.log"
tail -f "$TEST_DIR/logs/consumer.log"
tail -f "$TEST_DIR/logs/playback.log"
```

### Success Criteria

A successful test should demonstrate:
- ✅ All backend services start without errors
- ✅ RabbitMQ queue is empty at test start
- ✅ Database is freshly created with no leftover data
- ✅ Playback creates expected files in output directory
- ✅ Agent detects file changes and sends data to API
- ✅ API receives and stores data in database
- ✅ Worker processes messages from queue
- ✅ No errors in any service logs
- ✅ Database contains expected entities after test
- ✅ **Filesystem counts match database counts**: EPU sessions → grids, GridSquares → gridsquare rows, MRC files → micrograph rows
- ✅ **No or minimal "No grid found" messages** in agent.log (indicates proper grid association)

## Cleanup and Teardown

### Stop Running Services
```bash
# Stop all test-related processes
pkill -f smartem_backend.api
pkill -f smartem_backend.consumer
pkill -f smartem_agent
pkill -f fsrecorder

# Verify processes stopped
pgrep -f "smartem_backend|smartem_agent|fsrecorder"
```

### Optional: Full Teardown
```bash
# Clean playback directory
rm -rf ../epu-test-dir

# Stop k3s cluster (if needed)
./tools/dev-k8s.sh down
```

## Troubleshooting

### Database Connection Issues
```bash
# Verify k3s postgres is accessible
kubectl get pods -n smartem-decisions
kubectl logs deployment/postgres -n smartem-decisions

# Test connection
psql -h localhost -p 30432 -U username -d smartem_db -c "\l"
```

### RabbitMQ Connection Issues
```bash
# Check RabbitMQ pod status
kubectl get pods -n smartem-decisions
kubectl logs deployment/rabbitmq -n smartem-decisions

# Verify connection from host
curl -u guest:guest http://localhost:30673  # pragma: allowlist secret
```

### Agent Not Detecting Files
- Ensure `--api-url http://localhost:8000` points to correct API endpoint
- Check agent logs for connection errors
- Verify playback is writing to the directory agent is watching
- Confirm file permissions allow agent to read files

### Consumer Not Processing Messages
- Verify `RABBITMQ_URL=amqp://guest:guest@localhost:30672/` is correct  <!-- pragma: allowlist secret -->
- Check queue has messages waiting
- Review consumer logs for connection errors
- Ensure database is accessible to worker

## Known Issues and Notes

**RabbitMQ Queue Persistence**:
- Queue `smartem_backend` is durable and persists messages across connections
- **If multiple workers are started, they may compete for messages** - verify this is intended behaviour
- Restarting RabbitMQ service is currently the cleanest way to ensure empty queue between tests
- Consider implementing programmatic queue purging if restart is too slow

## Notes for Future Refinement

This document will evolve as we:
- Complete missing command details
- Identify optimal test parameters
- Discover edge cases during testing
- Convert manual steps into automation scripts
- Potentially create custom slash commands for common workflows
